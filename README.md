# Project Page for ME-VLM

## Abstract 

ME-VLM: Visual-Language Model for Micro-expression Synthesis and Classification
Micro-expression (ME) recognition is a challenging task due to the subtle and transient nature of these facial movements. Existing real-world ME datasets are limited in scale and diversity, hindering the development of robust recognition models. In this work, we introduce ME-VLM, a large-scale synthetic dataset for micro-expression analysis, generated using the image-to-video model. By leveraging a structured causal modeling approach, we employ Facial Action Units (AUs) as intermediate representations that drive the generation of realistic ME sequences. Our dataset enables the training of deep learning models for ME classification, and we demonstrate its effectiveness by fine-tuning a CLIP-based model for emotion recognition. Experimental results show that training on ME-VLM, followed by cross-validation evaluation, significantly improves classification accuracy compared to existing datasets. The controlled generation process ensures high fidelity, diverse identity coverage, and precise emotion annotation, making ME-VLM a valuable resource for advancing micro-expression research.
# vqa-me
